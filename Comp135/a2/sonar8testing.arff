%
% Note: original labels
%{ Rock, Mine}
% changed to -1,1
% for comp135 assignment
%
% NAME: Sonar, Mines vs. Rocks
% 
% SUMMARY: This is the data set used by Gorman and Sejnowski in their study
% of the classification of sonar signals using a neural network [1].  The
% task is to train a network to discriminate between sonar signals bounced
% off a metal cylinder and those bounced off a roughly cylindrical rock.
% 
% SOURCE: The data set was contributed to the benchmark collection by Terry
% Sejnowski, now at the Salk Institute and the University of California at
% San Deigo.  The data set was developed in collaboration with R. Paul
% Gorman of Allied-Signal Aerospace Technology Center.
% 
% MAINTAINER: Scott E. Fahlman
% 
% PROBLEM DESCRIPTION:
% 
% The file "sonar.mines" contains 111 patterns obtained by bouncing sonar
% signals off a metal cylinder at various angles and under various
% conditions.  The file "sonar.rocks" contains 97 patterns obtained from
% rocks under similar conditions.  The transmitted sonar signal is a
% frequency-modulated chirp, rising in frequency.  The data set contains
% signals obtained from a variety of different aspect angles, spanning 90
% degrees for the cylinder and 180 degrees for the rock.
% 
% Each pattern is a set of 60 numbers in the range 0.0 to 1.0.  Each number
% represents the energy within a particular frequency band, integrated over
% a certain period of time.  The integration aperture for higher frequencies
% occur later in time, since these frequencies are transmitted later during
% the chirp.
% 
% The label associated with each record contains the letter "R" if the object
% is a rock and "M" if it is a mine (metal cylinder).  The numbers in the
% labels are in increasing order of aspect angle, but they do not encode the
% angle directly.
% 
% METHODOLOGY: 
% 
% This data set can be used in a number of different ways to test learning
% speed, quality of ultimate learning, ability to generalize, or combinations
% of these factors.
% 
% In [1], Gorman and Sejnowski report two series of experiments: an
% "aspect-angle independent" series, in which the whole data set is used
% without controlling for aspect angle, and an "aspect-angle dependent"
% series in which the training and testing sets were carefully controlled to
% ensure that each set contained cases from each aspect angle in
% appropriate proportions.
% 
% For the aspect-angle independent experiments the combined set of 208 cases
% is divided randomly into 13 disjoint sets with 16 cases in each.  For each
% experiment, 12 of these sets are used as training data, while the 13th is
% reserved for testing.  The experiment is repeated 13 times so that every
% case appears once as part of a test set.  The reported performance is an
% average over the entire set of 13 different test sets, each run 10 times.
% 
% It was observed that this random division of the sample set led to rather
% uneven performance.  A few of the splits gave poor results, presumably
% because the test set contains some samples from aspect angles that are
% under-represented in the corresponding training set.  This motivated Gorman
% and Sejnowski to devise a different set of experiments in which an attempt
% was made to balance the training and test sets so that each would have a
% representative number of samples from all aspect angles.  Since detailed
% aspect angle information was not present in the data base of samples, the
% 208 samples were first divided into clusters, using a 60-dimensional
% Euclidian metric; each of these clusters was then divided between the
% 104-member training set and the 104-member test set.  
% 
% The actual training and testing samples used for the "aspect angle
% dependent" experiments are marked in the data files.  The reported
% performance is an average over 10 runs with this single division of the
% data set.
% 
% A standard back-propagation network was used for all experiments.  The
% network had 60 inputs and 2 output units, one indicating a cylinder and the
% other a rock.  Experiments were run with no hidden units (direct
% connections from each input to each output) and with a single hidden layer
% with 2, 3, 6, 12, or 24 units.  Each network was trained by 300 epochs over
% the entire training set.
% 
% The weight-update formulas used in this study were slightly different from
% the standard form.  A learning rate of 2.0 and momentum of 0.0 was used.
% Errors less than 0.2 were treated as zero.  Initial weights were uniform
% random values in the range -0.3 to +0.3.
% 
% RESULTS: 
% 
% For the angle independent experiments, Gorman and Sejnowski report the
% following results for networks with different numbers of hidden units:
% 
% Hidden	% Right on	Std.	% Right on	Std.
% Units	Training set	Dev.	Test Set	Dev.
% ------	------------	----	----------	----
% 0	89.4		2.1	77.1		8.3
% 2	96.5		0.7	81.9		6.2
% 3	98.8		0.4	82.0		7.3
% 6	99.7		0.2	83.5		5.6
% 12	99.8		0.1	84.7		5.7
% 24	99.8		0.1	84.5		5.7
% 
% For the angle-dependent experiments Gorman and Sejnowski report the
% following results:
% 
% Hidden	% Right on	Std.	% Right on	Std.
% Units	Training set	Dev.	Test Set	Dev.
% ------	------------	----	----------	----
% 0	79.3		3.4	73.1		4.8
% 2	96.2		2.2	85.7		6.3
% 3	98.1		1.5	87.6		3.0
% 6	99.4		0.9	89.3		2.4
% 12	99.8		0.6	90.4		1.8
% 24     100.0		0.0	89.2		1.4
% 
% Not surprisingly, the network's performance on the test set was somewhat
% better when the aspect angles in the training and test sets were balanced.
% 
% Gorman and Sejnowski further report that a nearest neighbor classifier on
% the same data gave an 82.7% probability of correct classification.
% 
% Three trained human subjects were each tested on 100 signals, chosen at
% random from the set of 208 returns used to create this data set.  Their
% responses ranged between 88% and 97% correct.  However, they may have been
% using information from the raw sonar signal that is not preserved in the
% processed data sets presented here.
% 
% REFERENCES: 
% 
% 1. Gorman, R. P., and Sejnowski, T. J. (1988).  "Analysis of Hidden Units
% in a Layered Network Trained to Classify Sonar Targets" in Neural Networks,
% Vol. 1, pp. 75-89.
%
%
%
%
% Relabeled values in attribute 'Class'
%    From: R                       To: Rock                
%    From: M                       To: Mine                
%
@relation sonar
@attribute 'attribute_1' real
@attribute 'attribute_2' real
@attribute 'attribute_3' real
@attribute 'attribute_4' real
@attribute 'attribute_5' real
@attribute 'attribute_6' real
@attribute 'attribute_7' real
@attribute 'attribute_8' real
@attribute 'attribute_9' real
@attribute 'attribute_10' real
@attribute 'attribute_11' real
@attribute 'attribute_12' real
@attribute 'attribute_13' real
@attribute 'attribute_14' real
@attribute 'attribute_15' real
@attribute 'attribute_16' real
@attribute 'attribute_17' real
@attribute 'attribute_18' real
@attribute 'attribute_19' real
@attribute 'attribute_20' real
@attribute 'attribute_21' real
@attribute 'attribute_22' real
@attribute 'attribute_23' real
@attribute 'attribute_24' real
@attribute 'attribute_25' real
@attribute 'attribute_26' real
@attribute 'attribute_27' real
@attribute 'attribute_28' real
@attribute 'attribute_29' real
@attribute 'attribute_30' real
@attribute 'attribute_31' real
@attribute 'attribute_32' real
@attribute 'attribute_33' real
@attribute 'attribute_34' real
@attribute 'attribute_35' real
@attribute 'attribute_36' real
@attribute 'attribute_37' real
@attribute 'attribute_38' real
@attribute 'attribute_39' real
@attribute 'attribute_40' real
@attribute 'attribute_41' real
@attribute 'attribute_42' real
@attribute 'attribute_43' real
@attribute 'attribute_44' real
@attribute 'attribute_45' real
@attribute 'attribute_46' real
@attribute 'attribute_47' real
@attribute 'attribute_48' real
@attribute 'attribute_49' real
@attribute 'attribute_50' real
@attribute 'attribute_51' real
@attribute 'attribute_52' real
@attribute 'attribute_53' real
@attribute 'attribute_54' real
@attribute 'attribute_55' real
@attribute 'attribute_56' real
@attribute 'attribute_57' real
@attribute 'attribute_58' real
@attribute 'attribute_59' real
@attribute 'attribute_60' real
@attribute 'Class' { -1, 1}
@data
0.0099, 0.0484, 0.0299, 0.0297, 0.0652, 0.1077, 0.2363, 0.2385, 0.0075, 0.1882, 0.1456, 0.1892, 0.3176, 0.134, 0.2169, 0.2458, 0.2589, 0.2786, 0.2298, 0.0656, 0.1441, 0.1179, 0.1668, 0.1783, 0.2476, 0.257, 0.1036, 0.5356, 0.7124, 0.6291, 0.4756, 0.6015, 0.7208, 0.6234, 0.5725, 0.7523, 0.8712, 0.9252, 0.9709, 0.9297, 0.8995, 0.7911, 0.56, 0.2838, 0.4407, 0.5507, 0.4331, 0.2905, 0.1981, 0.0779, 0.0396, 0.0173, 0.0149, 0.0115, 0.0202, 0.0139, 0.0029, 0.016, 0.0106, 1.0,-1
0.0346, 0.0509, 0.0079, 0.0243, 0.0432, 0.0735, 0.0938, 0.1134, 0.1228, 0.1508, 0.1809, 0.239, 0.2947, 0.2866, 0.401, 0.5325, 0.5486, 0.5823, 0.6041, 0.6749, 0.7084, 0.789, 0.9284, 0.9781, 0.9738, 1.0, 0.9702, 0.9956, 0.8235, 0.602, 0.5342, 0.4867, 0.3526, 0.1566, 0.0946, 0.1613, 0.2824, 0.339, 0.3019, 0.2945, 0.2978, 0.2676, 0.2055, 0.2069, 0.1625, 0.1216, 0.1013, 0.0744, 0.0386, 0.005, 0.0146, 0.004, 0.0122, 0.0107, 0.0112, 0.0102, 0.0052, 0.0024, 0.0079, 1.0,1
0.0261, 0.0266, 0.0223, 0.0749, 0.1364, 0.1513, 0.1316, 0.1654, 0.1864, 0.2013, 0.289, 0.365, 0.351, 0.3495, 0.4325, 0.5398, 0.6237, 0.6876, 0.7329, 0.8107, 0.8396, 0.8632, 0.8747, 0.9607, 0.9716, 0.9121, 0.8576, 0.8798, 0.772, 0.5711, 0.4264, 0.286, 0.3114, 0.2066, 0.1165, 0.0185, 0.1302, 0.248, 0.1637, 0.1103, 0.2144, 0.2033, 0.1887, 0.137, 0.1376, 0.0307, 0.0373, 0.0606, 0.0399, 0.0169, 0.0135, 0.0222, 0.0175, 0.0127, 0.0022, 0.0124, 0.0054, 0.0021, 0.0028, 1.0,1
0.0363, 0.0478, 0.0298, 0.021, 0.1409, 0.1916, 0.1349, 0.1613, 0.1703, 0.1444, 0.1989, 0.2154, 0.2863, 0.357, 0.398, 0.4359, 0.5334, 0.6304, 0.6995, 0.7435, 0.8379, 0.8641, 0.9014, 0.9432, 0.9536, 1.0, 0.9547, 0.9745, 0.8962, 0.7196, 0.5462, 0.3156, 0.2525, 0.1969, 0.2189, 0.1533, 0.0711, 0.1498, 0.1755, 0.2276, 0.1322, 0.1056, 0.1973, 0.1692, 0.1881, 0.1177, 0.0779, 0.0495, 0.0492, 0.0194, 0.025, 0.0115, 0.019, 0.0055, 0.0096, 0.005, 0.0066, 0.0114, 0.0073, 1.0,1
0.0126, 0.0149, 0.0641, 0.1732, 0.2565, 0.2559, 0.2947, 0.411, 0.4983, 0.592, 0.5832, 0.5419, 0.5472, 0.5314, 0.4981, 0.6985, 0.8292, 0.7839, 0.8215, 0.9363, 1.0, 0.9224, 0.7839, 0.547, 0.4562, 0.5922, 0.5448, 0.3971, 0.0882, 0.2385, 0.2005, 0.0587, 0.2544, 0.2009, 0.0329, 0.1547, 0.1212, 0.2446, 0.3171, 0.3195, 0.3051, 0.0836, 0.1266, 0.1381, 0.1136, 0.0516, 0.0073, 0.0278, 0.0372, 0.0121, 0.0153, 0.0092, 0.0035, 0.0098, 0.0121, 0.0006, 0.0181, 0.0094, 0.0116, 1.0,-1
0.0249, 0.0119, 0.0277, 0.076, 0.1218, 0.1538, 0.1192, 0.1229, 0.2119, 0.2531, 0.2855, 0.2961, 0.3341, 0.4287, 0.5205, 0.6087, 0.7236, 0.7577, 0.7726, 0.8098, 0.8995, 0.9247, 0.9365, 0.9853, 0.9776, 1.0, 0.9896, 0.9076, 0.7306, 0.5758, 0.4469, 0.3719, 0.2079, 0.0955, 0.0488, 0.1406, 0.2554, 0.2054, 0.1614, 0.2232, 0.1773, 0.2293, 0.2521, 0.1464, 0.0673, 0.0965, 0.1492, 0.1128, 0.0463, 0.0193, 0.014, 0.0027, 0.0068, 0.015, 0.0012, 0.0133, 0.0048, 0.0244, 0.0077, 1.0,1
0.0115, 0.015, 0.0136, 0.0076, 0.0211, 0.1058, 0.1023, 0.044, 0.0931, 0.0734, 0.074, 0.0622, 0.1055, 0.1183, 0.1721, 0.2584, 0.3232, 0.3817, 0.4243, 0.4217, 0.4449, 0.4075, 0.3306, 0.4012, 0.4466, 0.5218, 0.7552, 0.9503, 1.0, 0.9084, 0.8283, 0.7571, 0.7262, 0.6152, 0.568, 0.5757, 0.5324, 0.3672, 0.1669, 0.0866, 0.0646, 0.1891, 0.2683, 0.2887, 0.2341, 0.1668, 0.1015, 0.1195, 0.0704, 0.0167, 0.0107, 0.0091, 0.0016, 0.0084, 0.0064, 0.0026, 0.0029, 0.0037, 0.007, 1.0,-1
0.0192, 0.0607, 0.0378, 0.0774, 0.1388, 0.0809, 0.0568, 0.0219, 0.1037, 0.1186, 0.1237, 0.1601, 0.352, 0.4479, 0.3769, 0.5761, 0.6426, 0.679, 0.7157, 0.5466, 0.5399, 0.6362, 0.7849, 0.7756, 0.578, 0.4862, 0.4181, 0.2457, 0.0716, 0.0613, 0.1816, 0.4493, 0.5976, 0.3785, 0.2495, 0.5771, 0.8852, 0.8409, 0.357, 0.3133, 0.6096, 0.6378, 0.2709, 0.1419, 0.126, 0.1288, 0.079, 0.0829, 0.052, 0.0216, 0.036, 0.0331, 0.0131, 0.012, 0.0108, 0.0024, 0.0045, 0.0037, 0.0112, 1.0,-1
0.0293, 0.0644, 0.039, 0.0173, 0.0476, 0.0816, 0.0993, 0.0315, 0.0736, 0.086, 0.0414, 0.0472, 0.0835, 0.0938, 0.1466, 0.0809, 0.1179, 0.2179, 0.3326, 0.3258, 0.2111, 0.2302, 0.3361, 0.4259, 0.4609, 0.2606, 0.0874, 0.2862, 0.5606, 0.8344, 0.8096, 0.725, 0.8048, 0.9435, 1.0, 0.896, 0.5516, 0.3037, 0.2338, 0.2382, 0.3318, 0.3821, 0.1575, 0.2228, 0.1582, 0.1433, 0.1634, 0.1133, 0.0567, 0.0133, 0.017, 0.0035, 0.0052, 0.0083, 0.0078, 0.0075, 0.0105, 0.016, 0.0095, 1.0,-1
0.0228, 0.0853, 0.1, 0.0428, 0.1117, 0.1651, 0.1597, 0.2116, 0.3295, 0.3517, 0.333, 0.3643, 0.402, 0.4731, 0.5196, 0.6573, 0.8426, 0.8476, 0.8344, 0.8453, 0.7999, 0.8537, 0.9642, 1.0, 0.9357, 0.9409, 0.907, 0.7104, 0.632, 0.5667, 0.3501, 0.2447, 0.1698, 0.329, 0.3674, 0.2331, 0.2413, 0.2556, 0.1892, 0.194, 0.3074, 0.2785, 0.0308, 0.1238, 0.1854, 0.1753, 0.1079, 0.0728, 0.0242, 0.0191, 0.0159, 0.0172, 0.0191, 0.026, 0.014, 0.0125, 0.0116, 0.0093, 0.0012, 1.0,1
0.0209, 0.0261, 0.012, 0.0768, 0.1064, 0.168, 0.3016, 0.346, 0.3314, 0.4125, 0.3943, 0.1334, 0.4622, 0.997, 0.9137, 0.8292, 0.6994, 0.7825, 0.8789, 0.8501, 0.892, 0.9473, 1.0, 0.8975, 0.7806, 0.8321, 0.6502, 0.4548, 0.4732, 0.3391, 0.2747, 0.0978, 0.0477, 0.1403, 0.1834, 0.2148, 0.1271, 0.1912, 0.3391, 0.3444, 0.2369, 0.1195, 0.2665, 0.2587, 0.1393, 0.1083, 0.1383, 0.1321, 0.1069, 0.0325, 0.0316, 0.0057, 0.0159, 0.0085, 0.0372, 0.0101, 0.0127, 0.0288, 0.0129, 1.0,1
0.0473, 0.0509, 0.0819, 0.1252, 0.1783, 0.307, 0.3008, 0.2362, 0.383, 0.3759, 0.3021, 0.2909, 0.2301, 0.1411, 0.1582, 0.243, 0.4474, 0.5964, 0.6744, 0.7969, 0.8319, 0.7813, 0.8626, 0.7369, 0.4122, 0.2596, 0.3392, 0.3788, 0.4488, 0.6281, 0.7449, 0.7328, 0.7704, 0.787, 0.6048, 0.586, 0.6385, 0.7279, 0.6286, 0.5316, 0.4069, 0.1791, 0.1625, 0.2527, 0.1903, 0.1643, 0.0604, 0.0209, 0.0436, 0.0175, 0.0107, 0.0193, 0.0118, 0.0064, 0.0042, 0.0054, 0.0049, 0.0082, 0.0028, 1.0,-1
0.0162, 0.0041, 0.0239, 0.0441, 0.063, 0.0921, 0.1368, 0.1078, 0.1552, 0.1779, 0.2164, 0.2568, 0.3089, 0.3829, 0.4393, 0.5335, 0.5996, 0.6728, 0.7309, 0.8092, 0.8941, 0.9668, 1.0, 0.9893, 0.9376, 0.8991, 0.9184, 0.9128, 0.7811, 0.6018, 0.3765, 0.33, 0.228, 0.0212, 0.1117, 0.1788, 0.2373, 0.2843, 0.2241, 0.2715, 0.3363, 0.2546, 0.1867, 0.216, 0.1278, 0.0768, 0.107, 0.0946, 0.0636, 0.0227, 0.0128, 0.0173, 0.0135, 0.0114, 0.0062, 0.0157, 0.0088, 0.0036, 0.0053, 1.0,1
0.0388, 0.0324, 0.0688, 0.0898, 0.1267, 0.1515, 0.2134, 0.2613, 0.2832, 0.2718, 0.3645, 0.3934, 0.3843, 0.4677, 0.5364, 0.4823, 0.4835, 0.5862, 0.7579, 0.6997, 0.6918, 0.8633, 0.9107, 0.9346, 0.7884, 0.8585, 0.9261, 0.708, 0.5779, 0.5215, 0.4505, 0.3129, 0.1448, 0.1046, 0.182, 0.1519, 0.1017, 0.1438, 0.1986, 0.2039, 0.2778, 0.2879, 0.1331, 0.114, 0.131, 0.1433, 0.0624, 0.01, 0.0098, 0.0131, 0.0152, 0.0255, 0.0071, 0.0263, 0.0079, 0.0111, 0.0107, 0.0068, 0.0097, 1.0,1
0.0715, 0.0849, 0.0587, 0.0218, 0.0862, 0.1801, 0.1916, 0.1896, 0.296, 0.4186, 0.4867, 0.5249, 0.5959, 0.6855, 0.8573, 0.9718, 0.8693, 0.8711, 0.8954, 0.9922, 0.898, 0.8158, 0.8373, 0.7541, 0.5893, 0.5488, 0.5643, 0.5406, 0.4783, 0.4439, 0.3698, 0.2574, 0.1478, 0.1743, 0.1229, 0.1588, 0.1803, 0.1436, 0.1667, 0.263, 0.2234, 0.1239, 0.0869, 0.2092, 0.1499, 0.0676, 0.0899, 0.0927, 0.0658, 0.0086, 0.0216, 0.0153, 0.0121, 0.0096, 0.0196, 0.0042, 0.0066, 0.0099, 0.0083, 1.0,1
0.027, 0.0092, 0.0145, 0.0278, 0.0412, 0.0757, 0.1026, 0.1138, 0.0794, 0.152, 0.1675, 0.137, 0.1361, 0.1345, 0.2144, 0.5354, 0.683, 0.56, 0.3093, 0.3226, 0.443, 0.5573, 0.5782, 0.6173, 0.8132, 0.9819, 0.9823, 0.9166, 0.7423, 0.7736, 0.8473, 0.7352, 0.6671, 0.6083, 0.6239, 0.5972, 0.5715, 0.5242, 0.2924, 0.1536, 0.2003, 0.2031, 0.2207, 0.1778, 0.1353, 0.1373, 0.0749, 0.0472, 0.0325, 0.0179, 0.0045, 0.0084, 0.001, 0.0018, 0.0068, 0.0039, 0.012, 0.0132, 0.007, 1.0,-1
0.0664, 0.0575, 0.0842, 0.0372, 0.0458, 0.0771, 0.0771, 0.113, 0.2353, 0.1838, 0.2869, 0.4129, 0.3647, 0.1984, 0.284, 0.4039, 0.5837, 0.6792, 0.6086, 0.4858, 0.3246, 0.2013, 0.2082, 0.1686, 0.2484, 0.2736, 0.2984, 0.4655, 0.699, 0.7474, 0.7956, 0.7981, 0.6715, 0.6942, 0.744, 0.8169, 0.8912, 1.0, 0.8753, 0.7061, 0.6803, 0.5898, 0.4618, 0.3639, 0.1492, 0.1216, 0.1306, 0.1198, 0.0578, 0.0235, 0.0135, 0.0141, 0.019, 0.0043, 0.0036, 0.0026, 0.0024, 0.0162, 0.0109, 1.0,-1
0.027, 0.0163, 0.0341, 0.0247, 0.0822, 0.1256, 0.1323, 0.1584, 0.2017, 0.2122, 0.221, 0.2399, 0.2964, 0.4061, 0.5095, 0.5512, 0.6613, 0.6804, 0.652, 0.6788, 0.7811, 0.8369, 0.8969, 0.9856, 1.0, 0.9395, 0.8917, 0.8105, 0.6828, 0.5572, 0.4301, 0.3339, 0.2035, 0.0798, 0.0809, 0.1525, 0.2626, 0.2456, 0.198, 0.2412, 0.2409, 0.1901, 0.2077, 0.1767, 0.1119, 0.0779, 0.1344, 0.096, 0.0598, 0.033, 0.0197, 0.0189, 0.0204, 0.0085, 0.0043, 0.0092, 0.0138, 0.0094, 0.0105, 1.0,1
0.0352, 0.0116, 0.0191, 0.0469, 0.0737, 0.1185, 0.1683, 0.1541, 0.1466, 0.2912, 0.2328, 0.2237, 0.247, 0.156, 0.3491, 0.3308, 0.2299, 0.2203, 0.2493, 0.4128, 0.3158, 0.6191, 0.5854, 0.3395, 0.2561, 0.5599, 0.8145, 0.6941, 0.6985, 0.866, 0.593, 0.3664, 0.675, 0.8697, 0.7837, 0.7552, 0.5789, 0.4713, 0.1252, 0.6087, 0.7322, 0.5977, 0.3431, 0.1803, 0.2378, 0.3424, 0.2303, 0.0689, 0.0216, 0.0469, 0.0426, 0.0346, 0.0158, 0.0154, 0.0109, 0.0048, 0.0095, 0.0015, 0.0073, 1.0,-1
